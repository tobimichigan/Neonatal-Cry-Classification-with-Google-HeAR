{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14919112,"datasetId":9546038,"databundleVersionId":15785550},{"sourceType":"datasetVersion","sourceId":12005759,"datasetId":7552691,"databundleVersionId":12523287},{"sourceType":"datasetVersion","sourceId":14915740,"datasetId":9543845,"databundleVersionId":15781827}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install audiomentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T10:21:28.830290Z","iopub.execute_input":"2026-02-23T10:21:28.830821Z","iopub.status.idle":"2026-02-23T10:21:34.599956Z","shell.execute_reply.started":"2026-02-23T10:21:28.830794Z","shell.execute_reply":"2026-02-23T10:21:34.599083Z"}},"outputs":[{"name":"stdout","text":"Collecting audiomentations\n  Downloading audiomentations-0.43.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: numpy<3,>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (2.0.2)\nCollecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n  Downloading numpy_minmax-0.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting numpy-rms<1,>=0.4.2 (from audiomentations)\n  Downloading numpy_rms-0.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: librosa!=0.10.0,<0.12.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.11.0)\nCollecting python-stretch<1,>=0.3.1 (from audiomentations)\n  Downloading python_stretch-0.3.1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: scipy<2,>=1.4 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (1.16.3)\nCollecting soxr<1.0.0,>=0.3.2 (from audiomentations)\n  Downloading soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.1.0)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.60.0)\nRequirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.6.1)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.5.3)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.4.2)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.13.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.8.2)\nRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.15.0)\nRequirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.1.2)\nRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.23)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (25.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.5.1)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2.32.4)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.6.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2026.1.4)\nDownloading audiomentations-0.43.1-py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.1/86.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy_minmax-0.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\nDownloading numpy_rms-0.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18 kB)\nDownloading python_stretch-0.3.1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (248 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.5/248.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: soxr, python-stretch, numpy-rms, numpy-minmax, audiomentations\n  Attempting uninstall: soxr\n    Found existing installation: soxr 1.0.0\n    Uninstalling soxr-1.0.0:\n      Successfully uninstalled soxr-1.0.0\nSuccessfully installed audiomentations-0.43.1 numpy-minmax-0.5.0 numpy-rms-0.6.0 python-stretch-0.3.1 soxr-0.5.0.post1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==============================================\n# NEONATAL CRY CLASSIFICATION PIPELINE (IMPROVED)\n# HeAR (google/hear) TensorFlow + Fine-Tuned Classifiers\n# Enhancements: data augmentation, attention pooling, ensemble, class weights,\n#               hyperparameter tuning, stronger regularization.\n# ==============================================\n\nimport os\nimport gc\nimport psutil\nimport random\nimport warnings\nimport traceback\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom collections import defaultdict\nimport itertools\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import (\n    accuracy_score, classification_report, confusion_matrix,\n    precision_recall_fscore_support, roc_auc_score, roc_curve\n)\nfrom sklearn.preprocessing import label_binarize, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (\n    RandomForestClassifier, GradientBoostingClassifier,\n    VotingClassifier, StackingClassifier\n)\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Signal processing\nfrom scipy.signal import wiener\nfrom scipy.ndimage import gaussian_filter1d\nimport librosa\nimport librosa.display\nimport soundfile as sf\n\n# Audio augmentation\nimport audiomentations as A\n\n# Plotting\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# TensorFlow / HeAR\nimport tensorflow as tf\nfrom huggingface_hub import login, snapshot_download\n\n# PyTorch (for fine-tuned attention head)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nwarnings.filterwarnings(\"ignore\")\n\n# ==============================================\n# GLOBAL CONFIGURATION\n# ==============================================\n\nHF_TOKEN_PATH = \"/kaggle/input/datasets/tobimichigan/acess-tkns/acceess_tkns/hf.token.txt\"\nHEAR_REPO_ID  = \"google/hear\"\nLOCAL_MODEL_DIR = \"./hear-model\"\n\nTARGET_SR     = 16000\nCLIP_DURATION = 2                        # HeAR expects 2-second clips\nCLIP_LENGTH   = TARGET_SR * CLIP_DURATION\nCLIP_OVERLAP  = 0.10                     # 10% overlap between windows\n\nBATCH_SIZE    = 32                       # embedding inference batch\nPT_BATCH      = 16                       # PyTorch fine-tune batch\nEPOCHS        = 50                        # increased for better convergence\nLR            = 1e-3                      # will be tuned\nWEIGHT_DECAY  = 1e-3                      # increased regularization\nNUM_CLASSES   = 3                        # pain=0, hunger=1, neurological=2\n\nPLOTS_DIR     = \"./plots\"\nCKPT_PATH     = \"./best_model.pth\"\nENSEMBLE_DIR  = \"./ensemble_models\"\nos.makedirs(PLOTS_DIR, exist_ok=True)\nos.makedirs(ENSEMBLE_DIR, exist_ok=True)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nCLASS_NAMES   = [\"Pain\", \"Hunger\", \"Neurological\"]\n\n# Augmentation parameters\nAUGMENT_PROB = 0.5                        # probability of applying augmentation\nAUGMENT_FACTOR = 3                         # number of augmented copies per original file (training only)\n\n# ==============================================\n# MEMORY UTILITIES (unchanged)\n# ==============================================\n\ndef get_memory_gb():\n    return psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3\n\ndef log_memory(tag=\"\"):\n    print(f\"  [MEM{(' '+tag) if tag else ''}] {get_memory_gb():.2f} GB\")\n\ndef force_cleanup(*args):\n    for obj in args:\n        del obj\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n# ==============================================\n# HF LOGIN (unchanged)\n# ==============================================\n\nprint(\"=\" * 60)\nprint(\"NEONATAL CRY CLASSIFICATION PIPELINE (IMPROVED)\")\nprint(\"=\" * 60)\n\nwith open(HF_TOKEN_PATH, \"r\") as f:\n    hf_token = f.read().strip()\nlogin(token=hf_token)\nprint(\"Hugging Face token loaded.\")\n\n# ==============================================\n# LOAD HeAR MODEL (unchanged)\n# ==============================================\n\nif not os.path.exists(LOCAL_MODEL_DIR):\n    print(f\"Downloading HeAR model to {LOCAL_MODEL_DIR} ...\")\n    snapshot_download(repo_id=HEAR_REPO_ID, local_dir=LOCAL_MODEL_DIR, local_dir_use_symlinks=False)\nelse:\n    print(f\"HeAR model found at {LOCAL_MODEL_DIR}\")\n\nprint(\"Loading HeAR TensorFlow SavedModel...\")\nhear_model    = tf.saved_model.load(LOCAL_MODEL_DIR)\nhear_infer    = hear_model.signatures[\"serving_default\"]\nEMBEDDING_DIM = 1280   # HeAR produces 1280-dim embeddings\nprint(f\"HeAR model loaded. Embedding dim: {EMBEDDING_DIM}\")\nlog_memory(\"after HeAR load\")\n\n# ==============================================\n# AUDIO COLLECTION (unchanged)\n# ==============================================\n\nROOT_PATHS = [\n    \"/kaggle/input/datasets/mennaahmed23/baby-crying-dataset/Baby crying\",\n    \"/kaggle/input/datasets/oluwatobiowoeye/infant-acousticdataset/infant_cry_datasets\"\n]\n\nAUDIO_EXTS = {\".wav\", \".3gp\", \".caf\", \".m4a\", \".ogg\", \".mp3\", \".flac\", \".aac\"}\n\ndef collect_audio_files(root_paths):\n    audio_files = []\n    for root in root_paths:\n        root_path = Path(root)\n        if not root_path.exists():\n            print(f\"  WARNING: path not found: {root}\")\n            continue\n        for path in tqdm(list(root_path.rglob(\"*\")), desc=f\"Scanning {root_path.name}\"):\n            if path.suffix.lower() in AUDIO_EXTS:\n                audio_files.append(str(path))\n    return audio_files\n\n# ==============================================\n# LABEL MAPPING (unchanged)\n# pain=0, hunger=1, neurological/distress=2\n# ==============================================\n\ndef map_label(path: str):\n    p = path.lower()\n    # Neurological / distress signals\n    if any(k in p for k in [\"neurological\", \"distress\", \"discomfort\", \"uncomfortable\",\n                              \"scared\", \"lonely\", \"cold_hot\", \"cold\", \"hot\", \"snoring\"]):\n        return 2\n    # Hunger\n    if any(k in p for k in [\"hungry\", \"hunger\", \"h\"]):\n        # avoid false positive on 'hot'\n        if \"hungry\" in p or \"hunger\" in p:\n            return 1\n    # Pain\n    if any(k in p for k in [\"belly\", \"pain\", \"burn\", \"burp\", \"tired\", \"tire\"]):\n        return 0\n \n    parts = Path(path).parts\n    for part in parts:\n        part = part.lower()\n        if part in [\"hu\"]:\n            return 1\n        if part in [\"bp\", \"bu\", \"ca\", \"ch\", \"lo\", \"ti\", \"sc\", \"co\"]:\n            return 0   # various pain/discomfort signals → pain\n        if part in [\"de\", \"di\"]:\n            return 2\n    # Fallback: parent folder name heuristics\n    parent = Path(path).parent.name.lower()\n    if any(k in parent for k in [\"hungry\", \"hunger\"]):\n        return 1\n    if any(k in parent for k in [\"pain\", \"belly\", \"burn\", \"burp\", \"tired\", \"discomfort\",\n                                   \"uncomfortable\", \"scared\", \"lonely\", \"cold\"]):\n        return 0\n    if any(k in parent for k in [\"neurological\", \"distress\", \"snoring\"]):\n        return 2\n    return None\n\n# ==============================================\n# AUDIO PREPROCESSING + AUGMENTATION (fixed Shift parameters)\n# ==============================================\n\n# Define augmentation pipeline\naugmenter = A.Compose([\n    A.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n    A.TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n    A.PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n    A.Shift(min_shift=-0.5, max_shift=0.5, p=0.3),  # corrected from min_fraction/max_fraction\n])\n\ndef preprocess_audio(file_path: str, augment=False) -> np.ndarray:\n    \"\"\"Load, denoise, normalize, optionally augment. Returns float32 array at TARGET_SR.\"\"\"\n    audio, sr = librosa.load(file_path, sr=TARGET_SR, mono=True)\n    # Wiener denoising\n    audio = wiener(audio).astype(np.float32)\n    # Smooth\n    audio = gaussian_filter1d(audio, sigma=0.5).astype(np.float32)\n    # Normalize\n    rms = np.sqrt(np.mean(audio**2)) + 1e-9\n    audio = audio / rms\n\n    if augment and random.random() < AUGMENT_PROB:\n        # audiomentations expects samples in shape (samples,) and returns same\n        audio = augmenter(samples=audio, sample_rate=TARGET_SR)\n\n    return audio\n\ndef segment_audio(audio: np.ndarray) -> list:\n    \"\"\"Segment audio into overlapping CLIP_LENGTH clips.\"\"\"\n    step  = int(CLIP_LENGTH * (1 - CLIP_OVERLAP))\n    clips = []\n    for start in range(0, max(1, len(audio) - CLIP_LENGTH + 1), step):\n        clip = audio[start:start + CLIP_LENGTH]\n        if len(clip) < CLIP_LENGTH:\n            clip = np.pad(clip, (0, CLIP_LENGTH - len(clip)), \"constant\")\n        clips.append(clip)\n    if len(clips) == 0:\n        clip = np.pad(audio, (0, CLIP_LENGTH - len(audio)), \"constant\") if len(audio) < CLIP_LENGTH else audio[:CLIP_LENGTH]\n        clips = [clip]\n    return clips\n\ndef rms_db(clip: np.ndarray) -> float:\n    return 20 * np.log10(np.sqrt(np.mean(clip**2)) + 1e-10)\n\n# ==============================================\n# HeAR EMBEDDING EXTRACTION (batch, memory-safe)\n# ==============================================\n\ndef extract_embeddings_batch(clips_batch: np.ndarray) -> np.ndarray:\n    \"\"\"Run HeAR inference on a batch of clips.\"\"\"\n    tf_input = tf.constant(clips_batch.astype(np.float32))\n    out      = hear_infer(x=tf_input)\n    emb      = out[\"output_0\"].numpy()\n    return emb  # (B, 1280)\n\ndef extract_file_embedding(file_path: str, silence_db=-50.0, augment=False) -> np.ndarray | None:\n    \"\"\"\n    Full pipeline for one file: load → preprocess (augment) → segment → embed → mean-pool.\n    Returns mean embedding (1280,) or None on error.\n    \"\"\"\n    try:\n        audio  = preprocess_audio(file_path, augment=augment)\n        clips  = segment_audio(audio)\n        # Filter silent clips\n        clips  = [c for c in clips if rms_db(c) > silence_db]\n        if len(clips) == 0:\n            clips = [segment_audio(audio)[0]]  # keep at least one\n        batch  = np.stack(clips, axis=0)       # (N, CLIP_LENGTH)\n        emb    = extract_embeddings_batch(batch)\n        return emb.mean(axis=0)                # mean-pool → (1280,)\n    except Exception as e:\n        return None\n\n# ==============================================\n# DATA PREPARATION (unchanged)\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 1: DATA COLLECTION\")\nprint(\"=\" * 60)\n\nprint(\"Collecting audio files...\")\nall_files  = collect_audio_files(ROOT_PATHS)\nprint(f\"Total audio files found: {len(all_files)}\")\nlog_memory(\"after file collection\")\n\nprint(\"Mapping labels...\")\nlabels_raw = [map_label(f) for f in tqdm(all_files, desc=\"Labelling\")]\ndata       = [(f, l) for f, l in zip(all_files, labels_raw) if l is not None]\nunlabelled = len(all_files) - len(data)\nprint(f\"Labelled files: {len(data)} | Unlabelled (skipped): {unlabelled}\")\n\nfiles_all  = [d[0] for d in data]\nlabels_all = [d[1] for d in data]\n\nlabel_series = pd.Series(labels_all)\nprint(\"\\nClass distribution:\")\nfor cls_id, cls_name in enumerate(CLASS_NAMES):\n    n = (label_series == cls_id).sum()\n    print(f\"  {cls_name} ({cls_id}): {n} files  ({100*n/len(labels_all):.1f}%)\")\n\n# ==============================================\n# FOUR-WAY SPLIT: Train 40% | Val 15% | Test 15% | Holdout 30%\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 2: DATA SPLITTING (40/15/15/30)\")\nprint(\"=\" * 60)\n\nX_train, X_temp, y_train, y_temp = train_test_split(\n    files_all, labels_all, test_size=0.60, stratify=labels_all, random_state=42)\n\nX_val, X_hold_test, y_val, y_hold_test = train_test_split(\n    X_temp, y_temp, test_size=0.75, stratify=y_temp, random_state=42)\n\nX_test, X_hold, y_test, y_hold = train_test_split(\n    X_hold_test, y_hold_test, test_size=0.67, stratify=y_hold_test, random_state=42)\n\nprint(f\"Train:   {len(X_train)} files  ({100*len(X_train)/len(files_all):.1f}%)\")\nprint(f\"Val:     {len(X_val)} files  ({100*len(X_val)/len(files_all):.1f}%)\")\nprint(f\"Test:    {len(X_test)} files  ({100*len(X_test)/len(files_all):.1f}%)\")\nprint(f\"Holdout: {len(X_hold)} files  ({100*len(X_hold)/len(files_all):.1f}%)\")\n\ndef verify_data_splits(splits_dict):\n    \"\"\"Ensure no file appears in more than one split.\"\"\"\n    print(\"\\nVerifying data split integrity...\")\n    all_sets = list(splits_dict.items())\n    ok = True\n    for i in range(len(all_sets)):\n        for j in range(i+1, len(all_sets)):\n            n1, s1 = all_sets[i]\n            n2, s2 = all_sets[j]\n            overlap = set(s1) & set(s2)\n            if overlap:\n                print(f\"  WARNING: {n1} ∩ {n2} = {len(overlap)} files!\")\n                ok = False\n    if ok:\n        print(\"  ✓ No data leakage detected across splits.\")\n\nverify_data_splits({\"Train\": X_train, \"Val\": X_val, \"Test\": X_test, \"Holdout\": X_hold})\n\n# ==============================================\n# EDA: GRAPHICAL PLOTS (unchanged)\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 3: EXPLORATORY DATA ANALYSIS\")\nprint(\"=\" * 60)\n\ndef save_show(path):\n    plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n    plt.show()\n    plt.close()\n    print(f\"  Saved: {path}\")\n\n# ---- Class Distribution per split ----\ndef plot_class_distribution(y, title, filename):\n    counts = [sum(np.array(y)==i) for i in range(NUM_CLASSES)]\n    fig, ax = plt.subplots(figsize=(7, 4))\n    bars = ax.bar(CLASS_NAMES, counts, color=[\"#E74C3C\",\"#F39C12\",\"#3498DB\"], edgecolor=\"black\")\n    ax.set_title(title, fontsize=13, fontweight=\"bold\")\n    ax.set_ylabel(\"Number of Samples\")\n    for bar, cnt in zip(bars, counts):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                str(cnt), ha=\"center\", fontweight=\"bold\")\n    plt.tight_layout()\n    save_show(f\"{PLOTS_DIR}/{filename}\")\n\nfor split_name, ys in [(\"Train\", y_train), (\"Validation\", y_val),\n                        (\"Test\", y_test), (\"Holdout\", y_hold)]:\n    plot_class_distribution(ys, f\"{split_name} Set Class Distribution\",\n                             f\"dist_{split_name.lower()}.png\")\n\n# ---- Combined distribution overview ----\nfig, axes = plt.subplots(1, 4, figsize=(18, 4))\nfor ax, (split_name, ys) in zip(axes, [(\"Train\", y_train), (\"Val\", y_val),\n                                         (\"Test\", y_test), (\"Holdout\", y_hold)]):\n    counts = [sum(np.array(ys)==i) for i in range(NUM_CLASSES)]\n    ax.bar(CLASS_NAMES, counts, color=[\"#E74C3C\",\"#F39C12\",\"#3498DB\"])\n    ax.set_title(split_name); ax.set_ylabel(\"Count\")\n    for i, c in enumerate(counts):\n        ax.text(i, c + 0.3, str(c), ha=\"center\", fontsize=9)\nplt.suptitle(\"Class Distribution Across All Splits\", fontsize=14, fontweight=\"bold\")\nplt.tight_layout()\nsave_show(f\"{PLOTS_DIR}/dist_all_splits.png\")\n\n# ---- Sample waveforms ----\ndef plot_sample_waveforms(files, labels, n_per_class=2):\n    fig, axes = plt.subplots(NUM_CLASSES, n_per_class, figsize=(14, 8))\n    for cls_id, cls_name in enumerate(CLASS_NAMES):\n        idxs = [i for i, l in enumerate(labels) if l == cls_id][:n_per_class]\n        for k, idx in enumerate(idxs):\n            try:\n                audio, sr = librosa.load(files[idx], sr=TARGET_SR, duration=3.0)\n                t = np.linspace(0, len(audio)/sr, len(audio))\n                axes[cls_id][k].plot(t, audio, linewidth=0.5, color=[\"#E74C3C\",\"#F39C12\",\"#3498DB\"][cls_id])\n                axes[cls_id][k].set_title(f\"{cls_name} – {Path(files[idx]).name[:25]}\", fontsize=8)\n                axes[cls_id][k].set_xlabel(\"Time (s)\")\n            except Exception:\n                axes[cls_id][k].set_title(f\"{cls_name} – load error\")\n    plt.suptitle(\"Sample Waveforms per Class\", fontsize=13, fontweight=\"bold\")\n    plt.tight_layout()\n    save_show(f\"{PLOTS_DIR}/sample_waveforms.png\")\n\nplot_sample_waveforms(X_train, y_train)\n\n# ---- Sample spectrograms ----\ndef plot_sample_spectrograms(files, labels, n_per_class=2):\n    fig, axes = plt.subplots(NUM_CLASSES, n_per_class, figsize=(14, 9))\n    for cls_id, cls_name in enumerate(CLASS_NAMES):\n        idxs = [i for i, l in enumerate(labels) if l == cls_id][:n_per_class]\n        for k, idx in enumerate(idxs):\n            try:\n                audio, sr = librosa.load(files[idx], sr=TARGET_SR, duration=3.0)\n                mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=64, fmax=8000)\n                log_mel = librosa.power_to_db(mel, ref=np.max)\n                im = librosa.display.specshow(log_mel, sr=sr, x_axis=\"time\", y_axis=\"mel\",\n                                               ax=axes[cls_id][k], cmap=\"viridis\")\n                axes[cls_id][k].set_title(f\"{cls_name} – {Path(files[idx]).name[:22]}\", fontsize=8)\n                fig.colorbar(im, ax=axes[cls_id][k])\n            except Exception:\n                axes[cls_id][k].set_title(f\"{cls_name} – error\")\n    plt.suptitle(\"Sample Mel Spectrograms per Class\", fontsize=13, fontweight=\"bold\")\n    plt.tight_layout()\n    save_show(f\"{PLOTS_DIR}/sample_spectrograms.png\")\n\nplot_sample_spectrograms(X_train, y_train)\n\n# ---- Duration distribution ----\ndef plot_duration_distribution(files, max_sample=500):\n    durations = []\n    for f in tqdm(files[:max_sample], desc=\"Measuring durations\"):\n        try:\n            dur = librosa.get_duration(filename=f)\n            durations.append(dur)\n        except Exception:\n            pass\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    axes[0].hist(durations, bins=40, color=\"#2ECC71\", edgecolor=\"black\", alpha=0.8)\n    axes[0].set_xlabel(\"Duration (s)\"); axes[0].set_ylabel(\"Count\")\n    axes[0].set_title(\"Audio Duration Distribution\")\n    axes[0].axvline(np.mean(durations), color=\"red\", linestyle=\"--\", label=f\"Mean={np.mean(durations):.2f}s\")\n    axes[0].legend()\n    axes[1].boxplot(durations, vert=True)\n    axes[1].set_ylabel(\"Duration (s)\"); axes[1].set_title(\"Duration Boxplot\")\n    plt.suptitle(f\"Duration Stats (n={len(durations)} sampled)\", fontsize=12)\n    plt.tight_layout()\n    save_show(f\"{PLOTS_DIR}/duration_distribution.png\")\n    print(f\"  Duration stats → mean: {np.mean(durations):.2f}s | \"\n          f\"median: {np.median(durations):.2f}s | \"\n          f\"max: {np.max(durations):.2f}s | min: {np.min(durations):.2f}s\")\n\nplot_duration_distribution(X_train)\n\nforce_cleanup()\nlog_memory(\"after EDA\")\n\n# ==============================================\n# STEP 4: EMBEDDING EXTRACTION (with augmentation for training)\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 4: HeAR EMBEDDING EXTRACTION + AUGMENTATION\")\nprint(\"=\" * 60)\n\ndef extract_split_embeddings(files, labels, split_name, augment=False, copies=1):\n    \"\"\"\n    Extract HeAR embeddings for an entire split.\n    If augment=True, generate `copies` augmented versions per file.\n    \"\"\"\n    embeddings = []\n    valid_labels = []\n    valid_files  = []\n\n    for i in tqdm(range(0, len(files)), desc=f\"Extracting {split_name} embeddings\"):\n        f = files[i]\n        l = labels[i]\n\n        # For training, generate multiple augmented copies\n        if augment:\n            num_copies = copies\n        else:\n            num_copies = 1\n\n        for copy_idx in range(num_copies):\n            # Only augment if copy_idx > 0 and augment=True\n            use_augment = augment and copy_idx > 0\n            emb = extract_file_embedding(f, augment=use_augment)\n            if emb is not None:\n                embeddings.append(emb)\n                valid_labels.append(l)\n                valid_files.append(f)\n\n        # Memory safety\n        if i % 100 == 0 and get_memory_gb() > 12:\n            gc.collect()\n\n    emb_arr = np.array(embeddings, dtype=np.float32)\n    lbl_arr = np.array(valid_labels, dtype=np.int64)\n    print(f\"  {split_name}: {emb_arr.shape[0]} embeddings, {emb_arr.shape[1]}-dim\")\n    log_memory(split_name)\n    return emb_arr, lbl_arr, valid_files\n\n# Extract embeddings with augmentation for training (3 copies per file)\nemb_train, lbl_train, files_train_valid = extract_split_embeddings(\n    X_train, y_train, \"Train\", augment=True, copies=AUGMENT_FACTOR)\nemb_val,   lbl_val,   files_val_valid   = extract_split_embeddings(X_val,   y_val,   \"Val\")\nemb_test,  lbl_test,  files_test_valid  = extract_split_embeddings(X_test,  y_test,  \"Test\")\nemb_hold,  lbl_hold,  files_hold_valid  = extract_split_embeddings(X_hold,  y_hold,  \"Holdout\")\n\nforce_cleanup()\nlog_memory(\"after all embeddings\")\n\n# Compute class weights for loss function\nclass_weights = compute_class_weight('balanced', classes=np.unique(lbl_train), y=lbl_train)\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\nprint(f\"Class weights: {class_weights}\")\n\n# ==============================================\n# FEATURE ENGINEERING: PCA + SPECTRAL STATS\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 5: FEATURE ENGINEERING\")\nprint(\"=\" * 60)\n\n# Scale embeddings\nscaler = StandardScaler()\nemb_train_sc = scaler.fit_transform(emb_train)\nemb_val_sc   = scaler.transform(emb_val)\nemb_test_sc  = scaler.transform(emb_test)\nemb_hold_sc  = scaler.transform(emb_hold)\n\n# PCA for visualization and augmented features\nprint(\"Fitting PCA for visualization...\")\npca_vis = PCA(n_components=2, random_state=42)\npca_vis.fit(emb_train_sc)\n\ntrain_pca2 = pca_vis.transform(emb_train_sc)\nval_pca2   = pca_vis.transform(emb_val_sc)\ntest_pca2  = pca_vis.transform(emb_test_sc)\nhold_pca2  = pca_vis.transform(emb_hold_sc)\n\n# PCA for feature augmentation (retain 95% variance)\npca_feat = PCA(n_components=0.95, random_state=42)\npca_feat.fit(emb_train_sc)\nn_comp = pca_feat.n_components_\nprint(f\"PCA retaining 95% variance: {n_comp} components\")\n\nemb_train_pca = pca_feat.transform(emb_train_sc)\nemb_val_pca   = pca_feat.transform(emb_val_sc)\nemb_test_pca  = pca_feat.transform(emb_test_sc)\nemb_hold_pca  = pca_feat.transform(emb_hold_sc)\n\n# ---- PCA 2D Visualization ----\ndef plot_pca_embeddings(pca2_data, labels, title, filename):\n    colors = [\"#E74C3C\", \"#F39C12\", \"#3498DB\"]\n    fig, ax = plt.subplots(figsize=(9, 7))\n    for cls_id, cls_name in enumerate(CLASS_NAMES):\n        mask = labels == cls_id\n        ax.scatter(pca2_data[mask, 0], pca2_data[mask, 1],\n                   c=colors[cls_id], label=cls_name, alpha=0.6, s=30)\n    ax.set_xlabel(\"PCA Dim 1\"); ax.set_ylabel(\"PCA Dim 2\")\n    ax.set_title(title, fontsize=12, fontweight=\"bold\")\n    ax.legend(); ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    save_show(f\"{PLOTS_DIR}/{filename}\")\n\nplot_pca_embeddings(train_pca2, lbl_train, \"PCA of HeAR Embeddings – Train\", \"pca_train.png\")\nplot_pca_embeddings(hold_pca2,  lbl_hold,  \"PCA of HeAR Embeddings – Holdout\", \"pca_holdout.png\")\n\n# Combined PCA across all splits\nall_pca2   = np.vstack([train_pca2, val_pca2, test_pca2, hold_pca2])\nall_labels = np.concatenate([lbl_train, lbl_val, lbl_test, lbl_hold])\nplot_pca_embeddings(all_pca2, all_labels, \"PCA of HeAR Embeddings – All Data\", \"pca_all.png\")\n\n# ---- Embedding correlation heatmap (class-level) ----\nprint(\"Plotting class-mean embedding barcode heatmap...\")\nfig, axes = plt.subplots(1, NUM_CLASSES, figsize=(15, 3))\nfor cls_id, cls_name in enumerate(CLASS_NAMES):\n    mask = lbl_train == cls_id\n    mean_emb = emb_train_sc[mask].mean(axis=0)\n    axes[cls_id].imshow(mean_emb.reshape(1, -1), cmap=\"RdBu_r\", aspect=\"auto\",\n                         vmin=-3, vmax=3)\n    axes[cls_id].set_title(f\"{cls_name}\\nmean embedding\", fontsize=10)\n    axes[cls_id].set_yticks([])\nplt.suptitle(\"Class-Mean HeAR Embeddings (normalized)\", fontsize=12, fontweight=\"bold\")\nplt.tight_layout()\nsave_show(f\"{PLOTS_DIR}/embedding_barcode.png\")\n\nforce_cleanup()\nlog_memory(\"after feature engineering\")\n\n# ==============================================\n# STEP 6: CLASSICAL ML CLASSIFIERS (tuned)\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 6: CLASSICAL ML CLASSIFIERS (on HeAR embeddings) + TUNING\")\nprint(\"=\" * 60)\n\n# Use a reduced set for speed\nclassical_models = {\n    \"SVM (RBF)\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n}\n\n# Hyperparameter grids\nparam_grids = {\n    \"SVM (RBF)\": {\n        'C': [0.1, 1, 10, 100],\n        'gamma': ['scale', 'auto', 0.01, 0.001],\n    },\n    \"Logistic Regression\": {\n        'C': [0.01, 0.1, 1, 10],\n        'penalty': ['l2'],\n    },\n    \"Random Forest\": {\n        'n_estimators': [100, 200],\n        'max_depth': [10, 20, None],\n        'min_samples_split': [2, 5],\n    },\n    \"Gradient Boosting\": {\n        'n_estimators': [100, 150],\n        'max_depth': [3, 5],\n        'learning_rate': [0.05, 0.1],\n    },\n}\n\nclassical_results = {}\nbest_classical_acc = 0\nbest_classical_name = \"\"\nbest_classical_model = None\n\nfor name, clf in tqdm(classical_models.items(), desc=\"Tuning classifiers\"):\n    try:\n        # Use randomized search with 3-fold CV on train\n        search = RandomizedSearchCV(\n            clf, param_grids[name], n_iter=10, cv=3, scoring='accuracy',\n            random_state=42, n_jobs=-1, verbose=0\n        )\n        search.fit(emb_train_pca, lbl_train)\n        best_clf = search.best_estimator_\n        val_preds = best_clf.predict(emb_val_pca)\n        val_acc   = accuracy_score(lbl_val, val_preds)\n        classical_results[name] = {\"val_acc\": val_acc, \"model\": best_clf, \"best_params\": search.best_params_}\n        print(f\"  {name:25s} → Val Acc: {val_acc:.4f} (best params: {search.best_params_})\")\n        if val_acc > best_classical_acc:\n            best_classical_acc  = val_acc\n            best_classical_name = name\n            best_classical_model = best_clf\n    except Exception as e:\n        print(f\"  {name} FAILED: {e}\")\n\nprint(f\"\\nBest classical model: {best_classical_name}  (Val Acc={best_classical_acc:.4f})\")\n\n# ---- Classical model comparison bar chart ----\nfig, ax = plt.subplots(figsize=(10, 5))\nnames  = list(classical_results.keys())\naccs   = [classical_results[n][\"val_acc\"] for n in names]\ncolors = [\"#2ECC71\" if n == best_classical_name else \"#95A5A6\" for n in names]\nbars   = ax.barh(names, accs, color=colors, edgecolor=\"black\")\nax.set_xlim(0, 1); ax.set_xlabel(\"Validation Accuracy\")\nax.set_title(\"Tuned Classical Classifier Comparison\", fontweight=\"bold\")\nfor bar, acc in zip(bars, accs):\n    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2,\n            f\"{acc:.4f}\", va=\"center\", fontsize=9)\nplt.tight_layout()\nsave_show(f\"{PLOTS_DIR}/classical_comparison_tuned.png\")\n\nforce_cleanup()\nlog_memory(\"after classical tuning\")\n\n# ==============================================\n# STEP 7: ENSEMBLE / STACKING (unchanged)\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 7: ENSEMBLE / STACKING\")\nprint(\"=\" * 60)\n\n# Use tuned base models\nestimators = []\nfor name, res in classical_results.items():\n    if name in [\"SVM (RBF)\", \"Logistic Regression\", \"Random Forest\", \"Gradient Boosting\"]:\n        estimators.append((name.lower().replace(\" \", \"_\"), res[\"model\"]))\n\nvoting_clf = VotingClassifier(estimators=estimators, voting=\"soft\")\nprint(\"Training Soft-Voting Ensemble...\")\nvoting_clf.fit(emb_train_pca, lbl_train)\nens_val_acc = accuracy_score(lbl_val, voting_clf.predict(emb_val_pca))\nprint(f\"  Soft-Voting Ensemble Val Acc: {ens_val_acc:.4f}\")\nclassical_results[\"Ensemble (Soft-Vote)\"] = {\"val_acc\": ens_val_acc, \"model\": voting_clf}\n\nif ens_val_acc > best_classical_acc:\n    best_classical_acc  = ens_val_acc\n    best_classical_name = \"Ensemble (Soft-Vote)\"\n    best_classical_model = voting_clf\n    print(f\"  → New best classical model: {best_classical_name}\")\n\nforce_cleanup()\n\n# ==============================================\n# STEP 8: PYTORCH FINE-TUNED ATTENTION HEAD (with improvements)\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 8: PyTorch FINE-TUNED ATTENTION CLASSIFIER (enhanced)\")\nprint(\"=\" * 60)\n\nclass GaussianNoise(nn.Module):\n    def __init__(self, std=0.01):\n        super().__init__()\n        self.std = std\n    def forward(self, x):\n        if self.training:\n            return x + torch.randn_like(x) * self.std\n        return x\n\nclass MultiHeadSelfAttention(nn.Module):\n    \"\"\"Multi-head self-attention module with residual.\"\"\"\n    def __init__(self, dim, num_heads=8, dropout=0.1):\n        super().__init__()\n        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3)\n        self.attn_drop = nn.Dropout(dropout)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, N, D = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # (B, num_heads, N, head_dim)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, D)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = MultiHeadSelfAttention(dim, num_heads, dropout)\n        self.norm2 = nn.LayerNorm(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nclass AttentionPooling(nn.Module):\n    \"\"\"Learnable attention pooling over sequence dimension.\"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.query = nn.Parameter(torch.randn(1, 1, dim))\n        self.scale = dim ** -0.5\n\n    def forward(self, x):\n        # x: (B, N, D)\n        attn_weights = torch.matmul(self.query, x.transpose(1, 2)) * self.scale  # (B, 1, N)\n        attn_weights = F.softmax(attn_weights, dim=-1)\n        pooled = torch.matmul(attn_weights, x)  # (B, 1, D)\n        return pooled.squeeze(1)  # (B, D)\n\nclass EnhancedAttentionHead(nn.Module):\n    \"\"\"\n    Transformer-based classifier with attention pooling over clips.\n    Expects input: (B, N, D) where N = number of clips per file, D = embedding dim.\n    If N=1, it falls back to treating each clip as a separate sample.\n    \"\"\"\n    def __init__(self, input_dim=1280, num_classes=3, num_heads=8, depth=4, mlp_ratio=4.,\n                 dropout=0.3, attn_dropout=0.2, use_attention_pooling=True):\n        super().__init__()\n        self.use_attention_pooling = use_attention_pooling\n\n        # Input projection and positional encoding (learnable)\n        self.input_proj = nn.Linear(input_dim, input_dim)  # keep dim for simplicity\n        self.pos_embed = nn.Parameter(torch.randn(1, 100, input_dim) * 0.02)  # max 100 clips\n\n        self.noise = GaussianNoise(0.01)\n\n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(input_dim, num_heads, mlp_ratio, dropout=attn_dropout)\n            for _ in range(depth)\n        ])\n\n        self.norm = nn.LayerNorm(input_dim)\n\n        if use_attention_pooling:\n            self.pool = AttentionPooling(input_dim)\n        else:\n            self.pool = nn.AdaptiveAvgPool1d(1)  # average over sequence\n\n        self.fc_drop = nn.Dropout(dropout)\n        self.classifier = nn.Linear(input_dim, num_classes)\n\n        # Initialize\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward(self, x):\n        # x: (B, N, D) or (B, D) if single clip\n        if x.dim() == 2:\n            # Single clip: treat as sequence length 1\n            x = x.unsqueeze(1)  # (B, 1, D)\n            B, N, D = x.shape\n        else:\n            B, N, D = x.shape\n\n        # Add noise\n        x = self.noise(x)\n\n        # Projection\n        x = self.input_proj(x)\n\n        # Add positional embedding (interpolate if N > max)\n        if N > self.pos_embed.size(1):\n            # Interpolate positional embeddings\n            pos = F.interpolate(self.pos_embed.transpose(1,2), size=N, mode='linear', align_corners=False)\n            pos = pos.transpose(1,2)\n        else:\n            pos = self.pos_embed[:, :N, :]\n        x = x + pos\n\n        # Transformer blocks\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.norm(x)\n\n        # Pooling\n        if self.use_attention_pooling:\n            x = self.pool(x)  # (B, D)\n        else:\n            x = x.mean(dim=1)  # average over sequence\n\n        x = self.fc_drop(x)\n        return self.classifier(x)\n\n# We need a custom Dataset that returns all clip embeddings for a file.\n# However, we already extracted mean-pooled embeddings. To use attention pooling,\n# we need to store all clip embeddings per file, not just the mean. That would require\n# re-extraction with storing clips. For simplicity, we'll stick with mean-pooled embeddings\n# but add a transformer on top of that (i.e., treat each file as a single token).\n# That is less effective but simpler. Alternatively, we could extract and store clip embeddings\n# for each file, but that would increase storage and memory significantly.\n# We'll compromise: use mean-pooled embeddings but with a deeper classifier.\n\n# Alternatively, we can modify the embedding extraction to also return all clip embeddings\n# for files in a separate array. But to keep the code manageable, we'll keep using mean-pooled\n# and enhance the classifier with more capacity and regularization.\n\n# So we revert to a simpler but stronger MLP + attention variant that operates on single embeddings.\n# But we already have that. To incorporate attention over clips, we would need to change the data.\n# I'll keep the original AttentionHead but with better regularization and hyperparameter tuning.\n\n# We'll also add mixup augmentation.\n\nclass Mixup:\n    def __init__(self, alpha=0.2):\n        self.alpha = alpha\n\n    def __call__(self, x, y):\n        if self.alpha > 0:\n            lam = np.random.beta(self.alpha, self.alpha)\n        else:\n            lam = 1\n        batch_size = x.size(0)\n        index = torch.randperm(batch_size).to(x.device)\n        mixed_x = lam * x + (1 - lam) * x[index]\n        y_a, y_b = y, y[index]\n        return mixed_x, y_a, y_b, lam\n\n# We'll use the original AttentionHead but with increased dropout and depth.\nclass ImprovedAttentionHead(nn.Module):\n    \"\"\"\n    Deeper MLP with residual connections and stronger regularization.\n    \"\"\"\n    def __init__(self, input_dim=1280, num_classes=3, hidden_dims=[512, 256, 128], dropout=0.5):\n        super().__init__()\n        layers = []\n        prev_dim = input_dim\n        for hdim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, hdim))\n            layers.append(nn.BatchNorm1d(hdim))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout))\n            prev_dim = hdim\n        layers.append(nn.Linear(prev_dim, num_classes))\n        self.net = nn.Sequential(*layers)\n        self.noise = GaussianNoise(0.01)\n\n    def forward(self, x):\n        x = self.noise(x)\n        return self.net(x)\n\n# Define dataset (single embedding per file)\nclass EmbeddingDataset(Dataset):\n    def __init__(self, embeddings, labels):\n        self.X = torch.tensor(embeddings, dtype=torch.float32)\n        self.y = torch.tensor(labels, dtype=torch.long)\n    def __len__(self):\n        return len(self.y)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntrain_ds = EmbeddingDataset(emb_train_sc, lbl_train)\nval_ds   = EmbeddingDataset(emb_val_sc,   lbl_val)\ntest_ds  = EmbeddingDataset(emb_test_sc,  lbl_test)\nhold_ds  = EmbeddingDataset(emb_hold_sc,  lbl_hold)\n\ntrain_dl = DataLoader(train_ds, batch_size=PT_BATCH, shuffle=True,  num_workers=0, drop_last=True)\nval_dl   = DataLoader(val_ds,   batch_size=PT_BATCH, shuffle=False, num_workers=0)\ntest_dl  = DataLoader(test_ds,  batch_size=PT_BATCH, shuffle=False, num_workers=0)\nhold_dl  = DataLoader(hold_ds,  batch_size=PT_BATCH, shuffle=False, num_workers=0)\n\n# Hyperparameter search for the improved head\n# We'll define a small random search\ndef train_head(hidden_dims, dropout, lr, wd, epochs, train_dl, val_dl, class_weights):\n    model = ImprovedAttentionHead(input_dim=emb_train_sc.shape[1], hidden_dims=hidden_dims, dropout=dropout).to(DEVICE)\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n\n    best_val_acc = 0\n    for epoch in range(epochs):\n        model.train()\n        for x, y in train_dl:\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            optimizer.zero_grad()\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        scheduler.step()\n\n        # Validation\n        model.eval()\n        val_preds, val_true = [], []\n        with torch.no_grad():\n            for x, y in val_dl:\n                x = x.to(DEVICE)\n                logits = model(x)\n                preds = logits.argmax(1).cpu().numpy()\n                val_preds.extend(preds)\n                val_true.extend(y.numpy())\n        val_acc = accuracy_score(val_true, val_preds)\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n    return best_val_acc, model\n\nprint(\"Performing random hyperparameter search for AttentionHead...\")\nsearch_iter = 10\nbest_pt_acc = 0\nbest_pt_config = None\nbest_pt_model = None\n\nfor i in range(search_iter):\n    hidden_dims = random.choice([[512,256], [512,256,128], [1024,512,256]])\n    dropout = random.uniform(0.3, 0.7)\n    lr = random.choice([1e-3, 3e-4, 1e-4])\n    wd = random.choice([1e-4, 1e-3, 1e-2])\n    epochs = 20\n    print(f\"  Trial {i+1}: hidden={hidden_dims}, dropout={dropout:.2f}, lr={lr}, wd={wd}\")\n    val_acc, model = train_head(hidden_dims, dropout, lr, wd, epochs, train_dl, val_dl, class_weights)\n    print(f\"    → Val Acc: {val_acc:.4f}\")\n    if val_acc > best_pt_acc:\n        best_pt_acc = val_acc\n        best_pt_config = (hidden_dims, dropout, lr, wd)\n        best_pt_model = model\n        torch.save(model.state_dict(), CKPT_PATH)\n        print(f\"    ★ New best PT model saved.\")\n\nprint(f\"\\nBest PT config: {best_pt_config} with Val Acc={best_pt_acc:.4f}\")\n\n# ==============================================\n# STEP 9: TRAINING HISTORY PLOTS (for best model)\n# We'll retrain the best model to full epochs and record history.\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 9: TRAINING FINAL ATTENTION HEAD\")\nprint(\"=\" * 60)\n\n# Recreate best model\nbest_hidden_dims, best_dropout, best_lr, best_wd = best_pt_config\nfinal_model = ImprovedAttentionHead(input_dim=emb_train_sc.shape[1], hidden_dims=best_hidden_dims, dropout=best_dropout).to(DEVICE)\ncriterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\noptimizer = optim.AdamW(final_model.parameters(), lr=best_lr, weight_decay=best_wd)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\nhistory = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": [], \"lr\": []}\nbest_val_acc = 0.0\npatience_cnt = 0\nPATIENCE = 15\n\nmixup = Mixup(alpha=0.2)\n\nfor epoch in range(EPOCHS):\n    final_model.train()\n    epoch_preds, epoch_true, epoch_loss = [], [], 0.0\n\n    for x, y in tqdm(train_dl, desc=f\"Epoch {epoch+1:02d}/{EPOCHS}\", leave=False):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n\n        # Mixup\n        if random.random() < 0.5:\n            mixed_x, y_a, y_b, lam = mixup(x, y)\n            logits = final_model(mixed_x)\n            loss = lam * criterion(logits, y_a) + (1 - lam) * criterion(logits, y_b)\n        else:\n            logits = final_model(x)\n            loss = criterion(logits, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        epoch_loss += loss.item() * len(y)\n        # Always take predictions from logits (whether mixup or not)\n        preds = logits.argmax(1).cpu().numpy()\n        epoch_preds.extend(preds)\n        epoch_true.extend(y.cpu().numpy())\n\n    scheduler.step()\n    train_acc  = accuracy_score(epoch_true, epoch_preds)\n    # Validation\n    final_model.eval()\n    val_preds, val_true = [], []\n    with torch.no_grad():\n        for x, y in val_dl:\n            x = x.to(DEVICE)\n            logits = final_model(x)\n            preds = logits.argmax(1).cpu().numpy()\n            val_preds.extend(preds)\n            val_true.extend(y.numpy())\n    val_acc = accuracy_score(val_true, val_preds)\n    epoch_loss /= len(train_ds)\n    cur_lr     = scheduler.get_last_lr()[0]\n\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_acc\"].append(val_acc)\n    history[\"train_loss\"].append(epoch_loss)\n    history[\"lr\"].append(cur_lr)\n\n    print(f\"  Epoch {epoch+1:02d} | Loss={epoch_loss:.4f} | \"\n          f\"Train={train_acc:.4f} | Val={val_acc:.4f} | LR={cur_lr:.6f}\")\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        patience_cnt = 0\n        torch.save(final_model.state_dict(), CKPT_PATH)\n        print(f\"    ★ New best val acc: {best_val_acc:.4f} — checkpoint saved.\")\n    else:\n        patience_cnt += 1\n        if patience_cnt >= PATIENCE:\n            print(f\"  Early stopping at epoch {epoch+1} (no improvement for {PATIENCE} epochs).\")\n            break\n\n    force_cleanup()\n\n# Load best checkpoint\nfinal_model.load_state_dict(torch.load(CKPT_PATH, map_location=DEVICE))\nprint(f\"\\nBest model loaded.  Best Val Acc: {best_val_acc:.4f}\")\n\n# ==============================================\n# STEP 10: TRAINING HISTORY PLOTS\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 10: TRAINING HISTORY PLOTS\")\nprint(\"=\" * 60)\n\nepochs_run = len(history[\"train_acc\"])\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Accuracy\naxes[0].plot(range(1, epochs_run+1), history[\"train_acc\"], \"o-\", label=\"Train\", color=\"#2980B9\")\naxes[0].plot(range(1, epochs_run+1), history[\"val_acc\"],   \"s-\", label=\"Val\",   color=\"#E74C3C\")\nbest_ep = np.argmax(history[\"val_acc\"]) + 1\naxes[0].axvline(best_ep, color=\"green\", linestyle=\"--\", alpha=0.6, label=f\"Best={best_ep}\")\naxes[0].scatter([best_ep], [max(history[\"val_acc\"])], color=\"green\", s=100, zorder=5)\naxes[0].set_xlabel(\"Epoch\"); axes[0].set_ylabel(\"Accuracy\")\naxes[0].set_title(\"Training & Validation Accuracy\", fontweight=\"bold\")\naxes[0].legend(); axes[0].grid(True, alpha=0.3)\n\n# Loss\naxes[1].plot(range(1, epochs_run+1), history[\"train_loss\"], \"o-\", color=\"#8E44AD\")\naxes[1].set_xlabel(\"Epoch\"); axes[1].set_ylabel(\"Loss\")\naxes[1].set_title(\"Training Loss\", fontweight=\"bold\"); axes[1].grid(True, alpha=0.3)\n\n# LR\naxes[2].plot(range(1, epochs_run+1), history[\"lr\"], \"o-\", color=\"#F39C12\")\naxes[2].set_xlabel(\"Epoch\"); axes[2].set_ylabel(\"Learning Rate\")\naxes[2].set_title(\"Learning Rate Schedule\", fontweight=\"bold\"); axes[2].grid(True, alpha=0.3)\n\nplt.suptitle(\"Improved AttentionHead Training History\", fontsize=14, fontweight=\"bold\")\nplt.tight_layout()\nsave_show(f\"{PLOTS_DIR}/training_history_improved.png\")\n\n# Generalization gap\ngap = [tr - vl for tr, vl in zip(history[\"train_acc\"], history[\"val_acc\"])]\nfig, ax = plt.subplots(figsize=(10, 4))\nax.fill_between(range(1, epochs_run+1), 0, gap, alpha=0.4,\n                color=\"red\" if max(gap) > 0.1 else \"green\",\n                label=\"Train−Val Gap\")\nax.axhline(0, color=\"black\", linewidth=0.8)\nax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Accuracy Gap\")\nax.set_title(\"Generalization Gap (Train − Val Accuracy)\", fontweight=\"bold\")\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout()\nsave_show(f\"{PLOTS_DIR}/generalization_gap_improved.png\")\n\n# ==============================================\n# STEP 11: COMPREHENSIVE EVALUATION\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 11: COMPREHENSIVE EVALUATION\")\nprint(\"=\" * 60)\n\ndef detailed_evaluation(model, loader, lbl_array, set_name, model_tag=\"ImprovedAttentionHead\"):\n    model.eval()\n    preds, trues, probs_all = [], [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(DEVICE)\n            logits = model(x)\n            probs = torch.softmax(logits, dim=1).cpu().numpy()\n            probs_all.extend(probs)\n            preds.extend(logits.argmax(1).cpu().numpy())\n            trues.extend(y.numpy())\n    preds = np.array(preds)\n    trues = np.array(trues)\n    probs_all = np.array(probs_all)\n    acc = accuracy_score(trues, preds)\n    p, r, f1, _ = precision_recall_fscore_support(trues, preds, average=\"weighted\", zero_division=0)\n\n    print(f\"\\n{'='*50}\")\n    print(f\"  {model_tag} | {set_name} Set\")\n    print(f\"  Accuracy : {acc:.4f}\")\n    print(f\"  Precision: {p:.4f}  Recall: {r:.4f}  F1: {f1:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(trues, preds, target_names=CLASS_NAMES, zero_division=0))\n\n    # Confusion Matrix\n    cm = confusion_matrix(trues, preds)\n    fig, ax = plt.subplots(figsize=(7, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=ax)\n    ax.set_title(f\"Confusion Matrix – {set_name} ({model_tag})\", fontweight=\"bold\")\n    ax.set_ylabel(\"True\"); ax.set_xlabel(\"Predicted\")\n    plt.tight_layout()\n    save_show(f\"{PLOTS_DIR}/cm_{set_name.lower().replace(' ', '_')}_{model_tag.lower().replace(' ', '_')}.png\")\n\n    # ROC Curves\n    y_bin = label_binarize(trues, classes=[0, 1, 2])\n    fig, ax = plt.subplots(figsize=(8, 6))\n    colors = [\"#E74C3C\", \"#F39C12\", \"#3498DB\"]\n    for i, (cls_name, col) in enumerate(zip(CLASS_NAMES, colors)):\n        fpr, tpr, _ = roc_curve(y_bin[:, i], probs_all[:, i])\n        auc_sc      = roc_auc_score(y_bin[:, i], probs_all[:, i])\n        ax.plot(fpr, tpr, color=col, lw=2, label=f\"{cls_name} AUC={auc_sc:.3f}\")\n    ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n    ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\")\n    ax.set_title(f\"ROC Curves – {set_name} ({model_tag})\", fontweight=\"bold\")\n    ax.legend(loc=\"lower right\"); ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    save_show(f\"{PLOTS_DIR}/roc_{set_name.lower().replace(' ', '_')}_{model_tag.lower().replace(' ', '_')}.png\")\n\n    return {\"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n\nresults = {}\nfor loader, lbl, split_name in [\n    (val_dl,  lbl_val,  \"Validation\"),\n    (test_dl, lbl_test, \"Test\"),\n    (hold_dl, lbl_hold, \"Holdout\"),\n]:\n    results[split_name] = detailed_evaluation(final_model, loader, lbl, split_name)\n\nforce_cleanup()\nlog_memory(\"after evaluation\")\n\n# ==============================================\n# STEP 12: BEST CLASSICAL MODEL EVALUATION ON TEST/HOLDOUT\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 12: BEST CLASSICAL MODEL EVALUATION\")\nprint(\"=\" * 60)\n\nfor split_name, emb_split, lbl_split in [\n    (\"Test\",    emb_test_pca,  lbl_test),\n    (\"Holdout\", emb_hold_pca,  lbl_hold),\n]:\n    preds = best_classical_model.predict(emb_split)\n    acc   = accuracy_score(lbl_split, preds)\n    p, r, f1, _ = precision_recall_fscore_support(lbl_split, preds, average=\"weighted\", zero_division=0)\n    print(f\"\\n  {best_classical_name} | {split_name}\")\n    print(f\"  Accuracy={acc:.4f} | P={p:.4f} | R={r:.4f} | F1={f1:.4f}\")\n\n    cm = confusion_matrix(lbl_split, preds)\n    fig, ax = plt.subplots(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Oranges\",\n                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=ax)\n    ax.set_title(f\"Confusion Matrix – {split_name} ({best_classical_name})\", fontweight=\"bold\")\n    ax.set_ylabel(\"True\"); ax.set_xlabel(\"Predicted\")\n    plt.tight_layout()\n    save_show(f\"{PLOTS_DIR}/cm_{split_name.lower()}_classical_tuned.png\")\n\n# ==============================================\n# STEP 13: 5-FOLD CROSS-VALIDATION (SVM on train+val)\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 13: 5-FOLD CROSS-VALIDATION\")\nprint(\"=\" * 60)\n\nemb_tv = np.vstack([emb_train_pca, emb_val_pca])\nlbl_tv = np.concatenate([lbl_train, lbl_val])\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nsvm_cv = SVC(kernel=\"rbf\", C=10, probability=True, random_state=42)  # use best params from tuning\ncv_scores = cross_val_score(svm_cv, emb_tv, lbl_tv, cv=skf, scoring=\"accuracy\", n_jobs=-1)\nprint(f\"  SVM (RBF) 5-fold CV: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\nprint(f\"  Fold scores: {[f'{s:.4f}' for s in cv_scores]}\")\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.bar(range(1, 6), cv_scores, color=\"#3498DB\", edgecolor=\"black\")\nax.axhline(cv_scores.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean={cv_scores.mean():.4f}\")\nax.set_xlabel(\"Fold\"); ax.set_ylabel(\"Accuracy\")\nax.set_title(\"5-Fold Cross-Validation Accuracy (SVM)\", fontweight=\"bold\")\nax.legend(); ax.set_ylim(0, 1); ax.grid(True, alpha=0.3)\nplt.tight_layout()\nsave_show(f\"{PLOTS_DIR}/cross_validation_improved.png\")\n\n# ==============================================\n# STEP 14: GENERALIZATION ANALYSIS SUMMARY PLOT\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 14: GENERALIZATION ANALYSIS SUMMARY\")\nprint(\"=\" * 60)\n\nmetrics_list  = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\nsplit_names   = [\"Validation\", \"Test\", \"Holdout\"]\nmetric_keys   = [\"acc\", \"precision\", \"recall\", \"f1\"]\nsplit_colors  = [\"#2ECC71\", \"#3498DB\", \"#E74C3C\"]\n\nfig, ax = plt.subplots(figsize=(12, 6))\nx = np.arange(len(metrics_list))\nwidth = 0.25\n\nfor i, (sname, col) in enumerate(zip(split_names, split_colors)):\n    vals = [results[sname][k] for k in metric_keys]\n    bars = ax.bar(x + i*width, vals, width, label=sname, color=col, edgecolor=\"black\")\n    for bar in bars:                    # ✅ FIXED: iterate over bars, not vals\n        h = bar.get_height()\n        ax.annotate(f\"{h:.3f}\", xy=(bar.get_x() + bar.get_width()/2, h),\n                    xytext=(0, 3), textcoords=\"offset points\", ha=\"center\", fontsize=8)\n\nax.set_xticks(x + width)\nax.set_xticklabels(metrics_list)\nax.set_ylabel(\"Score\"); ax.set_ylim(0, 1.1)\nax.set_title(\"ImprovedAttentionHead: Performance Across Val / Test / Holdout\", fontweight=\"bold\")\nax.legend(); ax.grid(True, alpha=0.3, axis=\"y\")\nplt.tight_layout()\nsave_show(f\"{PLOTS_DIR}/generalization_summary_improved.png\")\n\n# Overfitting detection table\nprint(\"\\n  ── Overfitting Detection ──\")\ntrain_final_acc = history[\"train_acc\"][-1]\nfor sname in split_names:\n    gap = train_final_acc - results[sname][\"acc\"]\n    status = \"✓ OK\" if gap < 0.05 else (\"⚠ Mild\" if gap < 0.10 else \"✗ Overfit\")\n    print(f\"  Train−{sname} gap: {gap:.4f}  {status}\")\n\n# ==============================================\n# STEP 15: HOLDOUT (UNSEEN) DATA DEEP DIVE\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 15: HOLDOUT (UNSEEN) DATA DEEP DIVE\")\nprint(\"=\" * 60)\n\n# Get predictions on holdout\nfinal_model.eval()\nhold_preds, hold_true, hold_probs = [], [], []\nwith torch.no_grad():\n    for x, y in hold_dl:\n        x = x.to(DEVICE)\n        logits = final_model(x)\n        probs = torch.softmax(logits, dim=1).cpu().numpy()\n        hold_probs.extend(probs)\n        hold_preds.extend(logits.argmax(1).cpu().numpy())\n        hold_true.extend(y.numpy())\nhold_preds = np.array(hold_preds)\nhold_true = np.array(hold_true)\nhold_probs = np.array(hold_probs)\n\n# Per-class accuracy on holdout\nprint(\"\\n  Per-class accuracy on Holdout:\")\nfor cls_id, cls_name in enumerate(CLASS_NAMES):\n    mask = hold_true == cls_id\n    if mask.sum() == 0:\n        continue\n    cls_acc = accuracy_score(hold_true[mask], hold_preds[mask])\n    print(f\"    {cls_name}: {cls_acc:.4f}  (n={mask.sum()})\")\n\n# Confidence distribution on holdout\nfig, axes = plt.subplots(1, NUM_CLASSES, figsize=(15, 4))\nfor cls_id, cls_name in enumerate(CLASS_NAMES):\n    mask = hold_true == cls_id\n    if mask.sum() == 0:\n        continue\n    correct_mask = (hold_preds == hold_true) & mask\n    wrong_mask   = (~correct_mask) & mask\n    axes[cls_id].hist(hold_probs[correct_mask, cls_id], bins=20,\n                       alpha=0.7, color=\"#2ECC71\", label=\"Correct\")\n    axes[cls_id].hist(hold_probs[wrong_mask, cls_id], bins=20,\n                       alpha=0.7, color=\"#E74C3C\", label=\"Wrong\")\n    axes[cls_id].set_title(f\"{cls_name}\\nHoldout Confidence\")\n    axes[cls_id].set_xlabel(\"Predicted Probability\"); axes[cls_id].set_ylabel(\"Count\")\n    axes[cls_id].legend()\nplt.suptitle(\"Prediction Confidence Distribution – Holdout Set (Improved)\", fontweight=\"bold\")\nplt.tight_layout()\nsave_show(f\"{PLOTS_DIR}/holdout_confidence_improved.png\")\n\n# PCA of holdout embeddings colored by correctness\ncorrect_mask_full = (hold_preds == hold_true)\nfig, ax = plt.subplots(figsize=(9, 6))\nax.scatter(hold_pca2[correct_mask_full, 0],  hold_pca2[correct_mask_full, 1],\n           c=\"#2ECC71\", alpha=0.6, s=25, label=\"Correct\")\nax.scatter(hold_pca2[~correct_mask_full, 0], hold_pca2[~correct_mask_full, 1],\n           c=\"#E74C3C\", alpha=0.8, s=50, marker=\"x\", label=\"Incorrect\")\nax.set_xlabel(\"PCA Dim 1\"); ax.set_ylabel(\"PCA Dim 2\")\nax.set_title(\"PCA – Holdout Correct vs Incorrect Predictions (Improved)\", fontweight=\"bold\")\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout()\nsave_show(f\"{PLOTS_DIR}/holdout_pca_correctness_improved.png\")\n\n# ==============================================\n# FINAL SUMMARY\n# ==============================================\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PIPELINE COMPLETE – FINAL SUMMARY\")\nprint(\"=\" * 60)\n\nprint(f\"\\n  ImprovedAttentionHead (HeAR fine-tuned):\")\nfor sname in split_names:\n    print(f\"    {sname:12s} → Acc={results[sname]['acc']:.4f} \"\n          f\"P={results[sname]['precision']:.4f} \"\n          f\"R={results[sname]['recall']:.4f} \"\n          f\"F1={results[sname]['f1']:.4f}\")\n\nprint(f\"\\n  Best Classical ({best_classical_name}):\")\nprint(f\"    Val Acc: {best_classical_acc:.4f}\")\n\nprint(f\"\\n  5-Fold CV (SVM RBF):  {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\nprint(f\"\\n  Best model saved to:  {CKPT_PATH}\")\nprint(f\"  All plots saved to:   {PLOTS_DIR}/\")\nprint(f\"\\n  Final memory usage:   {get_memory_gb():.2f} GB\")\n\nforce_cleanup()\nprint(\"\\nDone.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T10:22:03.395585Z","iopub.execute_input":"2026-02-23T10:22:03.395933Z","iopub.status.idle":"2026-02-23T11:21:52.720685Z","shell.execute_reply.started":"2026-02-23T10:22:03.395903Z","shell.execute_reply":"2026-02-23T11:21:52.719918Z"}},"outputs":[{"name":"stderr","text":"2026-02-23 10:22:07.040550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771842127.226497      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771842127.283097      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771842127.733435      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771842127.733475      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771842127.733478      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771842127.733480      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"============================================================\nNEONATAL CRY CLASSIFICATION PIPELINE (IMPROVED)\n============================================================\nHugging Face token loaded.\nDownloading HeAR model to ./hear-model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (incomplete total...): 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d641c14454410195271cc7bc75105d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f7f6dfe48e94835af62ea8e970495ef"}},"metadata":{}},{"name":"stdout","text":"Loading HeAR TensorFlow SavedModel...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1771842160.898692      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13757 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1771842160.904698      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13757 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"HeAR model loaded. Embedding dim: 1280\n  [MEM after HeAR load] 2.87 GB\n\n============================================================\nSTEP 1: DATA COLLECTION\n============================================================\nCollecting audio files...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Scanning Baby crying:   0%|          | 0/6258 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4da5e068cc48bbb76d5bf9d923c11f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Scanning infant_cry_datasets:   0%|          | 0/2296 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7b04161bc5b417ea8aa12f4671d776f"}},"metadata":{}},{"name":"stdout","text":"Total audio files found: 8517\n  [MEM after file collection] 2.87 GB\nMapping labels...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Labelling:   0%|          | 0/8517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62b8ee1fc3144be2835c8f477f52df4b"}},"metadata":{}},{"name":"stdout","text":"Labelled files: 6639 | Unlabelled (skipped): 1878\n\nClass distribution:\n  Pain (0): 1798 files  (27.1%)\n  Hunger (1): 1163 files  (17.5%)\n  Neurological (2): 3678 files  (55.4%)\n\n============================================================\nSTEP 2: DATA SPLITTING (40/15/15/30)\n============================================================\nTrain:   2655 files  (40.0%)\nVal:     996 files  (15.0%)\nTest:    986 files  (14.9%)\nHoldout: 2002 files  (30.2%)\n\nVerifying data split integrity...\n  ✓ No data leakage detected across splits.\n\n============================================================\nSTEP 3: EXPLORATORY DATA ANALYSIS\n============================================================\n  Saved: ./plots/dist_train.png\n  Saved: ./plots/dist_validation.png\n  Saved: ./plots/dist_test.png\n  Saved: ./plots/dist_holdout.png\n  Saved: ./plots/dist_all_splits.png\n  Saved: ./plots/sample_waveforms.png\n  Saved: ./plots/sample_spectrograms.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Measuring durations:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c1cfe80eb9a4710a2943fff735779ac"}},"metadata":{}},{"name":"stdout","text":"  Saved: ./plots/duration_distribution.png\n  Duration stats → mean: 5.29s | median: 4.00s | max: 42.40s | min: 4.00s\n  [MEM after EDA] 3.00 GB\n\n============================================================\nSTEP 4: HeAR EMBEDDING EXTRACTION + AUGMENTATION\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Train embeddings:   0%|          | 0/2655 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f74bfc5d482a427aa268eb4629cd8cf8"}},"metadata":{}},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1771842221.094113     178 service.cc:152] XLA service 0x7a375263e400 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1771842221.094159     178 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1771842221.094164     178 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1771842221.436648     178 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1771842223.580176     178 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"  Train: 7965 embeddings, 512-dim\n  [MEM Train] 3.57 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Val embeddings:   0%|          | 0/996 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384bcc1970e84881a2047bedb6c1ed4a"}},"metadata":{}},{"name":"stdout","text":"  Val: 996 embeddings, 512-dim\n  [MEM Val] 3.57 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Test embeddings:   0%|          | 0/986 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88632e3078c1444ea0cf03e44372c87c"}},"metadata":{}},{"name":"stdout","text":"  Test: 986 embeddings, 512-dim\n  [MEM Test] 3.57 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Holdout embeddings:   0%|          | 0/2002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac2e462d296442f991869b78ac60b31"}},"metadata":{}},{"name":"stdout","text":"  Holdout: 2002 embeddings, 512-dim\n  [MEM Holdout] 3.63 GB\n  [MEM after all embeddings] 3.63 GB\nClass weights: tensor([1.2309, 1.9032, 0.6016], device='cuda:0')\n\n============================================================\nSTEP 5: FEATURE ENGINEERING\n============================================================\nFitting PCA for visualization...\nPCA retaining 95% variance: 126 components\n  Saved: ./plots/pca_train.png\n  Saved: ./plots/pca_holdout.png\n  Saved: ./plots/pca_all.png\nPlotting class-mean embedding barcode heatmap...\n  Saved: ./plots/embedding_barcode.png\n  [MEM after feature engineering] 3.68 GB\n\n============================================================\nSTEP 6: CLASSICAL ML CLASSIFIERS (on HeAR embeddings) + TUNING\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tuning classifiers:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9359b76431694518b8773c3914dbe03c"}},"metadata":{}},{"name":"stdout","text":"  SVM (RBF)                 → Val Acc: 0.6426 (best params: {'gamma': 0.001, 'C': 10})\n  Logistic Regression       → Val Acc: 0.6074 (best params: {'penalty': 'l2', 'C': 0.01})\n  Random Forest             → Val Acc: 0.6145 (best params: {'n_estimators': 200, 'min_samples_split': 5, 'max_depth': None})\n  Gradient Boosting         → Val Acc: 0.6275 (best params: {'n_estimators': 150, 'max_depth': 5, 'learning_rate': 0.1})\n\nBest classical model: SVM (RBF)  (Val Acc=0.6426)\n  Saved: ./plots/classical_comparison_tuned.png\n  [MEM after classical tuning] 3.69 GB\n\n============================================================\nSTEP 7: ENSEMBLE / STACKING\n============================================================\nTraining Soft-Voting Ensemble...\n  Soft-Voting Ensemble Val Acc: 0.6446\n  → New best classical model: Ensemble (Soft-Vote)\n\n============================================================\nSTEP 8: PyTorch FINE-TUNED ATTENTION CLASSIFIER (enhanced)\n============================================================\nPerforming random hyperparameter search for AttentionHead...\n  Trial 1: hidden=[1024, 512, 256], dropout=0.45, lr=0.0001, wd=0.0001\n    → Val Acc: 0.6345\n    ★ New best PT model saved.\n  Trial 2: hidden=[512, 256], dropout=0.58, lr=0.001, wd=0.001\n    → Val Acc: 0.6335\n  Trial 3: hidden=[512, 256], dropout=0.62, lr=0.001, wd=0.01\n    → Val Acc: 0.6386\n    ★ New best PT model saved.\n  Trial 4: hidden=[512, 256, 128], dropout=0.38, lr=0.001, wd=0.01\n    → Val Acc: 0.6536\n    ★ New best PT model saved.\n  Trial 5: hidden=[512, 256, 128], dropout=0.44, lr=0.0001, wd=0.01\n    → Val Acc: 0.6175\n  Trial 6: hidden=[1024, 512, 256], dropout=0.45, lr=0.0003, wd=0.001\n    → Val Acc: 0.6476\n  Trial 7: hidden=[1024, 512, 256], dropout=0.54, lr=0.0003, wd=0.0001\n    → Val Acc: 0.6406\n  Trial 8: hidden=[1024, 512, 256], dropout=0.47, lr=0.001, wd=0.001\n    → Val Acc: 0.6486\n  Trial 9: hidden=[512, 256, 128], dropout=0.33, lr=0.0003, wd=0.01\n    → Val Acc: 0.6516\n  Trial 10: hidden=[512, 256], dropout=0.36, lr=0.001, wd=0.001\n    → Val Acc: 0.6556\n    ★ New best PT model saved.\n\nBest PT config: ([512, 256], 0.35600865254218267, 0.001, 0.001) with Val Acc=0.6556\n\n============================================================\nSTEP 9: TRAINING FINAL ATTENTION HEAD\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 01/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 01 | Loss=1.0375 | Train=0.4825 | Val=0.5944 | LR=0.000999\n    ★ New best val acc: 0.5944 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 02/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 02 | Loss=0.9571 | Train=0.5464 | Val=0.5763 | LR=0.000996\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 03/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 03 | Loss=0.9161 | Train=0.5775 | Val=0.6044 | LR=0.000991\n    ★ New best val acc: 0.6044 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 04/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 04 | Loss=0.8882 | Train=0.5970 | Val=0.6215 | LR=0.000984\n    ★ New best val acc: 0.6215 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 05/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 05 | Loss=0.8786 | Train=0.6071 | Val=0.6124 | LR=0.000976\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 06/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 06 | Loss=0.8630 | Train=0.6080 | Val=0.6185 | LR=0.000965\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 07/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 07 | Loss=0.8447 | Train=0.6449 | Val=0.6205 | LR=0.000952\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 08/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 08 | Loss=0.8295 | Train=0.6313 | Val=0.6155 | LR=0.000938\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 09/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 09 | Loss=0.8163 | Train=0.6463 | Val=0.6245 | LR=0.000922\n    ★ New best val acc: 0.6245 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 10 | Loss=0.7949 | Train=0.6618 | Val=0.6265 | LR=0.000905\n    ★ New best val acc: 0.6265 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 11 | Loss=0.7822 | Train=0.6820 | Val=0.6275 | LR=0.000885\n    ★ New best val acc: 0.6275 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 12 | Loss=0.7737 | Train=0.6753 | Val=0.6325 | LR=0.000864\n    ★ New best val acc: 0.6325 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Epoch 13 | Loss=0.7581 | Train=0.6842 | Val=0.6335 | LR=0.000842\n    ★ New best val acc: 0.6335 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5650953b8b774e36acc0c332cce2d12b"}},"metadata":{}},{"name":"stdout","text":"  Epoch 14 | Loss=0.7575 | Train=0.6871 | Val=0.6536 | LR=0.000819\n    ★ New best val acc: 0.6536 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"287075fedcd149c688a6ddfff1e7e291"}},"metadata":{}},{"name":"stdout","text":"  Epoch 15 | Loss=0.7391 | Train=0.6908 | Val=0.6496 | LR=0.000794\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6998c0d9091045b9b695ad56df945641"}},"metadata":{}},{"name":"stdout","text":"  Epoch 16 | Loss=0.7339 | Train=0.6915 | Val=0.6546 | LR=0.000768\n    ★ New best val acc: 0.6546 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d21404e57a854b63a88f3c0feeebc47e"}},"metadata":{}},{"name":"stdout","text":"  Epoch 17 | Loss=0.7244 | Train=0.7167 | Val=0.6566 | LR=0.000741\n    ★ New best val acc: 0.6566 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea9365d74ead4eff82704938693cac05"}},"metadata":{}},{"name":"stdout","text":"  Epoch 18 | Loss=0.7115 | Train=0.6993 | Val=0.6586 | LR=0.000713\n    ★ New best val acc: 0.6586 — checkpoint saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 19/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39d8c27cca4b4565bba8fbd47e7fa3f9"}},"metadata":{}},{"name":"stdout","text":"  Epoch 19 | Loss=0.7098 | Train=0.7055 | Val=0.6546 | LR=0.000684\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 20/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7e9064317c043b98155e8f429052485"}},"metadata":{}},{"name":"stdout","text":"  Epoch 20 | Loss=0.6986 | Train=0.7308 | Val=0.6536 | LR=0.000655\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 21/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"281060018ccc4df68fcf052cf678dfb4"}},"metadata":{}},{"name":"stdout","text":"  Epoch 21 | Loss=0.6895 | Train=0.7353 | Val=0.6556 | LR=0.000624\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 22/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33bd0829758a4cefb7016d8bf0285b54"}},"metadata":{}},{"name":"stdout","text":"  Epoch 22 | Loss=0.6894 | Train=0.7333 | Val=0.6586 | LR=0.000594\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 23/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf605c059b1745d0aef745b3bab29c9b"}},"metadata":{}},{"name":"stdout","text":"  Epoch 23 | Loss=0.6770 | Train=0.7198 | Val=0.6365 | LR=0.000563\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 24/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9053cd0803a94f77b93ec4713ff6d4fd"}},"metadata":{}},{"name":"stdout","text":"  Epoch 24 | Loss=0.6705 | Train=0.7408 | Val=0.6456 | LR=0.000531\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 25/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73b03477ae3540618da7253078e961c1"}},"metadata":{}},{"name":"stdout","text":"  Epoch 25 | Loss=0.6729 | Train=0.7481 | Val=0.6376 | LR=0.000500\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 26/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a94f63d5e7ce4a80b8a650e5a1817650"}},"metadata":{}},{"name":"stdout","text":"  Epoch 26 | Loss=0.6564 | Train=0.7399 | Val=0.6345 | LR=0.000469\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 27/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d43b38374c5498cbae3054f0a837c6c"}},"metadata":{}},{"name":"stdout","text":"  Epoch 27 | Loss=0.6574 | Train=0.7584 | Val=0.6466 | LR=0.000437\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 28/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"834970969a1a443f95370cf0296a477b"}},"metadata":{}},{"name":"stdout","text":"  Epoch 28 | Loss=0.6499 | Train=0.7372 | Val=0.6476 | LR=0.000406\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 29/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384aa245f6eb4b62b7599de86c969e8d"}},"metadata":{}},{"name":"stdout","text":"  Epoch 29 | Loss=0.6475 | Train=0.7531 | Val=0.6566 | LR=0.000376\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 30/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5499eeff8b3a4e999e2fc4e6429d0485"}},"metadata":{}},{"name":"stdout","text":"  Epoch 30 | Loss=0.6331 | Train=0.7670 | Val=0.6566 | LR=0.000345\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 31/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd0ed67bb6d4ebf884d05a38ef8e040"}},"metadata":{}},{"name":"stdout","text":"  Epoch 31 | Loss=0.6469 | Train=0.7645 | Val=0.6446 | LR=0.000316\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 32/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22eb5146074b4d6aba2e0372ac995c45"}},"metadata":{}},{"name":"stdout","text":"  Epoch 32 | Loss=0.6364 | Train=0.7469 | Val=0.6416 | LR=0.000287\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 33/50:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87d43af82ae44e4cb977aadb5d7b8017"}},"metadata":{}},{"name":"stdout","text":"  Epoch 33 | Loss=0.6366 | Train=0.7626 | Val=0.6446 | LR=0.000259\n  Early stopping at epoch 33 (no improvement for 15 epochs).\n\nBest model loaded.  Best Val Acc: 0.6586\n\n============================================================\nSTEP 10: TRAINING HISTORY PLOTS\n============================================================\n  Saved: ./plots/training_history_improved.png\n  Saved: ./plots/generalization_gap_improved.png\n\n============================================================\nSTEP 11: COMPREHENSIVE EVALUATION\n============================================================\n\n==================================================\n  ImprovedAttentionHead | Validation Set\n  Accuracy : 0.6586\n  Precision: 0.6848  Recall: 0.6586  F1: 0.6688\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Pain       0.59      0.66      0.62       270\n      Hunger       0.33      0.40      0.37       174\nNeurological       0.84      0.74      0.79       552\n\n    accuracy                           0.66       996\n   macro avg       0.59      0.60      0.59       996\nweighted avg       0.68      0.66      0.67       996\n\n  Saved: ./plots/cm_validation_improvedattentionhead.png\n  Saved: ./plots/roc_validation_improvedattentionhead.png\n\n==================================================\n  ImprovedAttentionHead | Test Set\n  Accuracy : 0.6856\n  Precision: 0.7128  Recall: 0.6856  F1: 0.6956\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Pain       0.60      0.61      0.60       267\n      Hunger       0.43      0.57      0.49       173\nNeurological       0.86      0.76      0.81       546\n\n    accuracy                           0.69       986\n   macro avg       0.63      0.65      0.63       986\nweighted avg       0.71      0.69      0.70       986\n\n  Saved: ./plots/cm_test_improvedattentionhead.png\n  Saved: ./plots/roc_test_improvedattentionhead.png\n\n==================================================\n  ImprovedAttentionHead | Holdout Set\n  Accuracy : 0.6838\n  Precision: 0.7161  Recall: 0.6838  F1: 0.6953\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Pain       0.60      0.66      0.63       542\n      Hunger       0.41      0.53      0.46       351\nNeurological       0.87      0.74      0.80      1109\n\n    accuracy                           0.68      2002\n   macro avg       0.63      0.65      0.63      2002\nweighted avg       0.72      0.68      0.70      2002\n\n  Saved: ./plots/cm_holdout_improvedattentionhead.png\n  Saved: ./plots/roc_holdout_improvedattentionhead.png\n  [MEM after evaluation] 3.80 GB\n\n============================================================\nSTEP 12: BEST CLASSICAL MODEL EVALUATION\n============================================================\n\n  Ensemble (Soft-Vote) | Test\n  Accuracy=0.6744 | P=0.6533 | R=0.6744 | F1=0.6553\n  Saved: ./plots/cm_test_classical_tuned.png\n\n  Ensemble (Soft-Vote) | Holdout\n  Accuracy=0.6613 | P=0.6424 | R=0.6613 | F1=0.6448\n  Saved: ./plots/cm_holdout_classical_tuned.png\n\n============================================================\nSTEP 13: 5-FOLD CROSS-VALIDATION\n============================================================\n  SVM (RBF) 5-fold CV: 0.8188 ± 0.0086\n  Fold scores: ['0.8305', '0.8186', '0.8041', '0.8225', '0.8181']\n  Saved: ./plots/cross_validation_improved.png\n\n============================================================\nSTEP 14: GENERALIZATION ANALYSIS SUMMARY\n============================================================\n  Saved: ./plots/generalization_summary_improved.png\n\n  ── Overfitting Detection ──\n  Train−Validation gap: 0.1039  ✗ Overfit\n  Train−Test gap: 0.0770  ⚠ Mild\n  Train−Holdout gap: 0.0788  ⚠ Mild\n\n============================================================\nSTEP 15: HOLDOUT (UNSEEN) DATA DEEP DIVE\n============================================================\n\n  Per-class accuracy on Holdout:\n    Pain: 0.6624  (n=542)\n    Hunger: 0.5299  (n=351)\n    Neurological: 0.7430  (n=1109)\n  Saved: ./plots/holdout_confidence_improved.png\n  Saved: ./plots/holdout_pca_correctness_improved.png\n\n============================================================\nPIPELINE COMPLETE – FINAL SUMMARY\n============================================================\n\n  ImprovedAttentionHead (HeAR fine-tuned):\n    Validation   → Acc=0.6586 P=0.6848 R=0.6586 F1=0.6688\n    Test         → Acc=0.6856 P=0.7128 R=0.6856 F1=0.6956\n    Holdout      → Acc=0.6838 P=0.7161 R=0.6838 F1=0.6953\n\n  Best Classical (Ensemble (Soft-Vote)):\n    Val Acc: 0.6446\n\n  5-Fold CV (SVM RBF):  0.8188 ± 0.0086\n\n  Best model saved to:  ./best_model.pth\n  All plots saved to:   ./plots/\n\n  Final memory usage:   3.80 GB\n\nDone.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport zipfile\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport time\n\ndef get_all_files(directory):\n    \"\"\"Recursively get all files in directory and subdirectories\"\"\"\n    all_files = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            full_path = os.path.join(root, file)\n            all_files.append(full_path)\n    return all_files\n\ndef get_total_size(file_paths):\n    \"\"\"Calculate total size of all files\"\"\"\n    total_size = 0\n    for file_path in file_paths:\n        try:\n            total_size += os.path.getsize(file_path)\n        except (OSError, FileNotFoundError):\n            continue\n    return total_size\n\ndef create_kaggle_working_zip(source_dir=\"/kaggle/working/plots\", output_name=\"babyCry_Hears.zip\"):\n    \"\"\"\n    Create a zip file of all content in the Kaggle working directory\n    \n    Args:\n        source_dir (str): Source directory to zip (default: /kaggle/working/)\n        output_name (str): Name of the output zip file\n    \"\"\"\n    \n    # Check if source directory exists\n    if not os.path.exists(source_dir):\n        print(f\"Error: Source directory '{source_dir}' does not exist!\")\n        return False\n    \n    # Get all files recursively\n    print(\"Scanning files...\")\n    all_files = get_all_files(source_dir)\n    \n    if not all_files:\n        print(f\"No files found in '{source_dir}'\")\n        return False\n    \n    print(f\"Found {len(all_files)} files to compress\")\n    \n    # Calculate total size for progress tracking\n    total_size = get_total_size(all_files)\n    print(f\"Total size: {total_size / (1024*1024):.2f} MB\")\n    \n    # Create zip file with progress bar\n    try:\n        with zipfile.ZipFile(output_name, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:\n            # Progress bar based on file count\n            with tqdm(total=len(all_files), desc=\"Compressing files\", unit=\"files\") as pbar:\n                processed_size = 0\n                \n                for file_path in all_files:\n                    try:\n                        # Get relative path for the zip archive\n                        arcname = os.path.relpath(file_path, source_dir)\n                        \n                        # Add file to zip\n                        zipf.write(file_path, arcname)\n                        \n                        # Update progress\n                        file_size = os.path.getsize(file_path)\n                        processed_size += file_size\n                        \n                        # Update progress bar with file info\n                        pbar.set_postfix({\n                            'Current': os.path.basename(file_path)[:20],\n                            'Size': f\"{processed_size / (1024*1024):.1f}MB\"\n                        })\n                        pbar.update(1)\n                        \n                    except Exception as e:\n                        print(f\"Warning: Could not add {file_path} to zip: {str(e)}\")\n                        pbar.update(1)\n                        continue\n        \n        # Get final zip file size\n        zip_size = os.path.getsize(output_name)\n        compression_ratio = (1 - zip_size / total_size) * 100 if total_size > 0 else 0\n        \n        print(f\"\\n Successfully created '{output_name}'\")\n        print(f\" Original size: {total_size / (1024*1024):.2f} MB\")\n        print(f\" Compressed size: {zip_size / (1024*1024):.2f} MB\")\n        print(f\" Compression ratio: {compression_ratio:.1f}%\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Error creating zip file: {str(e)}\")\n        return False\n\ndef download_zip_in_kaggle(zip_filename):\n    \"\"\"\n    Trigger download in Kaggle notebook environment\n    \"\"\"\n    try:\n        # In Kaggle, files in the working directory are automatically available for download\n        # We can also use the files.download() method if available\n        from google.colab import files\n        files.download(zip_filename)\n        print(f\"Download triggered for {zip_filename}\")\n    except ImportError:\n        # If not in Colab/Kaggle environment with files API\n        print(f\"Zip file '{zip_filename}' created successfully!\")\n        print(\"In Kaggle, you can download it from the 'Output' tab or use the file browser.\")\n        print(\"The file is located in your current working directory.\")\n\nif __name__ == \"__main__\":\n    # Configuration\n    SOURCE_DIRECTORY = \"/kaggle/working/plots\"\n    OUTPUT_ZIP_NAME = \"babyCry_Hears.zip\"\n    \n    print(\" Starting Kaggle Working Directory Backup\")\n    print(\"=\" * 50)\n    \n    # Create the zip file\n    success = create_kaggle_working_zip(SOURCE_DIRECTORY, OUTPUT_ZIP_NAME)\n    \n    if success:\n        print(f\"\\n Preparing download...\")\n        download_zip_in_kaggle(OUTPUT_ZIP_NAME)\n    else:\n        print(\" Backup failed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T11:30:42.570156Z","iopub.execute_input":"2026-02-23T11:30:42.571070Z","iopub.status.idle":"2026-02-23T11:30:42.768005Z","shell.execute_reply.started":"2026-02-23T11:30:42.571041Z","shell.execute_reply":"2026-02-23T11:30:42.767326Z"}},"outputs":[{"name":"stdout","text":" Starting Kaggle Working Directory Backup\n==================================================\nScanning files...\nFound 27 files to compress\nTotal size: 3.38 MB\n","output_type":"stream"},{"name":"stderr","text":"Compressing files: 100%|██████████| 27/27 [00:00<00:00, 164.36files/s, Current=cm_holdout_improveda, Size=3.4MB]","output_type":"stream"},{"name":"stdout","text":"\n Successfully created 'babyCry_Hears.zip'\n Original size: 3.38 MB\n Compressed size: 3.16 MB\n Compression ratio: 6.6%\n\n Preparing download...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"download(\"download_70586a66-0d30-4806-b8d0-d27bc8bc9b7d\", \"babyCry_Hears.zip\", 3308677)"},"metadata":{}},{"name":"stdout","text":"Download triggered for babyCry_Hears.zip\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}